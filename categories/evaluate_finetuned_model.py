import logging
"""
EVALUATION SCRIPT Ð´Ð»Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Food101
=====================================

Ð†ÐÐ¡Ð¢Ð Ð£ÐšÐ¦Ð†Ð¯: Ð©Ð¾Ð± Ð²ÐºÐ°Ð·Ð°Ñ‚Ð¸ ÑˆÐ»ÑÑ… Ð´Ð¾ Ð¼Ð¾Ð´ÐµÐ»Ñ–, Ð¿ÐµÑ€ÐµÐ¹Ð´Ñ–Ñ‚ÑŒ Ð´Ð¾ Ñ€ÑÐ´ÐºÐ° ~270 Ñ– Ð·Ð¼Ñ–Ð½Ñ–Ñ‚ÑŒ:
model_path = "Ð²Ð°Ñˆ_ÑˆÐ»ÑÑ…_Ð´Ð¾_Ð¼Ð¾Ð´ÐµÐ»Ñ–.pth"

ÐŸÑ€Ð¸ÐºÐ»Ð°Ð´Ð¸:
- model_path = "food101_densenet_finetuned_final_dataset_256_20250609_082804.pth"
- model_path = "food101_densenet_finetuned_final_20250607_142912.pth" 
- model_path = "c:/full/path/to/your/model.pth"

Ð¡ÐºÑ€Ð¸Ð¿Ñ‚ Ð³ÐµÐ½ÐµÑ€ÑƒÑ”:
- confusion_matrix.png (Ð²Ñ–Ð·ÑƒÐ°Ð»ÑŒÐ½Ð° Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñ)
- confusion_matrix.csv (Ñ‚Ð°Ð±Ð»Ð¸Ñ‡Ð½Ð¸Ð¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚)
- evaluation_results.csv (Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ– Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸)
- top_confused_classes.png
- per_class_accuracy.png
"""

import torch
import torch.nn as nn
import torchvision
from torchvision import datasets, models, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score
from collections import OrderedDict
import os
import time
from datetime import datetime

def load_model(model_path, num_classes, device):
    """Load the fine-tuned model"""
    logging.info(f"Loading model from {model_path}")
    
    # Create model architecture (same as training)
    model = models.densenet121(weights=None)  # Don't load pretrained weights
    
    # Get the number of input features for the classifier
    in_features = model.classifier.in_features
    
    # Create the same classifier head as used in training
    classifier = nn.Sequential(OrderedDict([
        ('fc1', nn.Linear(in_features, 512)),
        ('relu', nn.ReLU()),
        ('dropout', nn.Dropout(0.5)),
        ('fc2', nn.Linear(512, num_classes)),
        ('output', nn.LogSoftmax(dim=1))
    ]))
    model.classifier = classifier
      # Load the trained weights
    model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))
    model = model.to(device)
    model.eval()
    
    logging.info("Model loaded successfully!")
    return model

def evaluate_model(model, dataloader, device, class_names):
    """Evaluate model and return predictions and true labels"""
    logging.info("Starting model evaluation...")
    model.eval()
    all_preds = []
    all_labels = []
    all_probs = []
    
    total_batches = len(dataloader)
    
    with torch.no_grad():
        for batch_idx, (inputs, labels) in enumerate(dataloader):
            inputs = inputs.to(device)
            labels = labels.to(device)
            
            outputs = model(inputs)
            probs = torch.exp(outputs)  # Convert log probabilities to probabilities
            _, preds = torch.max(outputs, 1)
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())
            
            if batch_idx % 20 == 0:
                logging.info(f'Processed batch {batch_idx+1}/{total_batches} ({100*batch_idx/total_batches:.1f}%)')
    
    logging.info("Model evaluation completed!")
    return np.array(all_preds), np.array(all_labels), np.array(all_probs)

def plot_confusion_matrix(y_true, y_pred, class_names, save_path=None):
    """Plot confusion matrix"""
    logging.info("Creating confusion matrix...")
    cm = confusion_matrix(y_true, y_pred)
    
    # For readability, if we have too many classes, show only top confused classes
    if len(class_names) > 20:
        plt.figure(figsize=(20, 16))
        sns.heatmap(cm, annot=False, fmt='d', cmap='Blues', 
                   xticklabels=False, yticklabels=False)
        plt.title('Confusion Matrix (All Classes)', fontsize=16)
    else:
        plt.figure(figsize=(15, 12))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=class_names, yticklabels=class_names)
        plt.title('Confusion Matrix', fontsize=16)
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
    
    plt.xlabel('Predicted Label', fontsize=12)
    plt.ylabel('True Label', fontsize=12)
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        logging.info(f"Confusion matrix saved to {save_path}")
    plt.show()
    
    return cm

def plot_top_confused_classes(y_true, y_pred, class_names, top_n=15, save_path=None):
    """Plot top confused class pairs"""
    logging.info("Creating top confused classes plot...")
    cm = confusion_matrix(y_true, y_pred)
    
    # Find top confused pairs (excluding diagonal)
    confused_pairs = []
    for i in range(len(class_names)):
        for j in range(len(class_names)):
            if i != j and cm[i, j] > 0:
                confused_pairs.append((class_names[i], class_names[j], cm[i, j]))
    
    # Sort by confusion count
    confused_pairs.sort(key=lambda x: x[2], reverse=True)
    top_confused = confused_pairs[:top_n]
    
    if top_confused:
        pairs = [f"{pair[0]} â†’ {pair[1]}" for pair in top_confused]
        counts = [pair[2] for pair in top_confused]
        
        plt.figure(figsize=(12, 8))
        bars = plt.barh(pairs, counts, color='coral')
        plt.xlabel('Number of Misclassifications')
        plt.title(f'Top {top_n} Most Confused Class Pairs')
        plt.gca().invert_yaxis()
        
        # Add value labels on bars
        for bar, count in zip(bars, counts):
            plt.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2, 
                    str(count), ha='left', va='center')
        
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logging.info(f"Top confused classes plot saved to {save_path}")
        plt.show()

def plot_class_accuracy(y_true, y_pred, class_names, save_path=None):
    """Plot per-class accuracy"""
    logging.info("Creating per-class accuracy plot...")
    cm = confusion_matrix(y_true, y_pred)
    class_accuracy = cm.diagonal() / cm.sum(axis=1)
    
    # Sort by accuracy
    sorted_indices = np.argsort(class_accuracy)
    sorted_classes = [class_names[i] for i in sorted_indices]
    sorted_accuracy = class_accuracy[sorted_indices]
    
    plt.figure(figsize=(15, max(10, len(class_names) * 0.4)))
    colors = ['red' if acc < 0.5 else 'orange' if acc < 0.7 else 'green' for acc in sorted_accuracy]
    bars = plt.barh(range(len(sorted_classes)), sorted_accuracy, color=colors)
    
    plt.yticks(range(len(sorted_classes)), sorted_classes, fontsize=8)
    plt.xlabel('Accuracy')
    plt.title('Per-Class Accuracy')
    plt.xlim(0, 1)
    plt.grid(axis='x', alpha=0.3)
    
    # Add accuracy values on bars
    for i, (bar, acc) in enumerate(zip(bars, sorted_accuracy)):
        plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, 
                f'{acc:.3f}', ha='left', va='center', fontsize=7)
    
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        logging.info(f"Per-class accuracy plot saved to {save_path}")
    plt.show()
    
    return class_accuracy

def save_results_to_csv(y_true, y_pred, class_names, class_accuracy, save_path):
    """Save detailed results to CSV"""
    logging.info("Saving results to CSV...")
    
    # Calculate per-class metrics
    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True, zero_division=0)
    
    # Overall metrics
    overall_accuracy = accuracy_score(y_true, y_pred)
    overall_f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)
    overall_f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)
    
    # Create detailed DataFrame
    results_data = []
    
    # Add overall metrics
    results_data.append({
        'Class': 'OVERALL',
        'Accuracy': overall_accuracy,
        'Precision': report['macro avg']['precision'],
        'Recall': report['macro avg']['recall'],
        'F1_Score': overall_f1_macro,
        'F1_Score_Weighted': overall_f1_weighted,
        'Support': int(report['macro avg']['support'])
    })
    
    # Add per-class metrics
    for i, class_name in enumerate(class_names):
        if class_name in report:
            results_data.append({
                'Class': class_name,
                'Accuracy': class_accuracy[i],
                'Precision': report[class_name]['precision'],
                'Recall': report[class_name]['recall'],
                'F1_Score': report[class_name]['f1-score'],
                'F1_Score_Weighted': overall_f1_weighted,  # Same for all classes
                'Support': int(report[class_name]['support'])
            })
    
    df = pd.DataFrame(results_data)
    df.to_csv(save_path, index=False)
    
    logging.info(f"Results saved to {save_path}")
    logging.info("\n=== OVERALL RESULTS ===")
    logging.info(f"Overall Accuracy: {overall_accuracy:.4f}")
    logging.info(f"Macro F1 Score: {overall_f1_macro:.4f}")
    logging.info(f"Weighted F1 Score: {overall_f1_weighted:.4f}")
    
    return df

def save_confusion_matrix_to_csv(y_true, y_pred, class_names, save_path):
    """Save confusion matrix to CSV file"""
    logging.info("Saving confusion matrix to CSV...")
    
    # Generate confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    
    # Create DataFrame with class names as both index and columns
    cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)
    
    # Add row and column totals
    cm_df['Total_Predicted'] = cm_df.sum(axis=1)
    cm_df.loc['Total_Actual'] = cm_df.sum(axis=0)
    
    # Save to CSV
    cm_df.to_csv(save_path)
    
    logging.info(f"Confusion matrix saved to {save_path}")
    return cm_df

def main():
    logging.info("=== Food101 Fine-tuned Model Evaluation ===")
    logging.info(f"Evaluation started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
      # Setup
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logging.info(f"Using device: {device}")    
    # Paths - use original Food101 dataset (all categories)
    base_path = os.path.dirname(os.path.abspath(__file__))
    
    # ===================================================================
    # Ð’ÐšÐÐ—ÐÐ¢Ð˜ Ð¨Ð›Ð¯Ð¥ Ð”Ðž ÐœÐžÐ”Ð•Ð›Ð† Ð¢Ð£Ð¢ (Ð·Ð¼Ñ–Ð½Ñ–Ñ‚ÑŒ Ð½Ð° Ð²Ð°Ñˆ Ñ„Ð°Ð¹Ð» .pth):
    # ===================================================================
    model_path = "../models/food_256_finetuned.pth"

    # ÐœÐ¾Ð¶Ð»Ð¸Ð²Ñ– Ð²Ð°Ñ€Ñ–Ð°Ð½Ñ‚Ð¸:
    # model_path = "../models/food_256_finetuned.pth"       # Ð”Ð»Ñ dataset 256
    # model_path = "./food_256_finetuned.pth"               # Ð¯ÐºÑ‰Ð¾ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð² Ñ‚Ñ–Ð¹ Ð¶Ðµ Ð¿Ð°Ð¿Ñ†Ñ–
    # model_path = "models/food_256_finetuned.pth"          # Ð¯ÐºÑ‰Ð¾ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð² Ð¿Ð°Ð¿Ñ†Ñ– models
    # ===================================================================
    
    # ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡Ð½Ðµ Ð²Ð¸Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ Ñ‚Ð¸Ð¿Ñƒ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ñƒ Ð· Ð½Ð°Ð·Ð²Ð¸ Ð¼Ð¾Ð´ÐµÐ»Ñ–
    model_filename = os.path.basename(model_path)
    if "dataset_256" in model_filename.lower():
        dataset_type = "256"
        expected_classes = 256
        logging.info("ðŸŽ¯ Ð’Ð¸ÑÐ²Ð»ÐµÐ½Ð¾ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ñƒ 256 ÐºÐ»Ð°ÑÑ–Ð²")
    else:
        dataset_type = "101"
        expected_classes = 101
        logging.info("ðŸŽ¯ Ð’Ð¸ÑÐ²Ð»ÐµÐ½Ð¾ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ñƒ Food-101 (101 ÐºÐ»Ð°Ñ)")
      # ÐÐ°Ð»Ð°ÑˆÑ‚ÑƒÐ²Ð°Ð½Ð½Ñ ÑˆÐ»ÑÑ…Ñ–Ð² Ð´Ð¾ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ñƒ Ð·Ð°Ð»ÐµÐ¶Ð½Ð¾ Ð²Ñ–Ð´ Ñ‚Ð¸Ð¿Ñƒ
    if dataset_type == "256":
        # Ð”Ð»Ñ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ñƒ 256 ÐºÐ»Ð°ÑÑ–Ð² Ð¿Ð¾Ñ‚Ñ€Ñ–Ð±ÐµÐ½ ÑÐ¿ÐµÑ†Ð¸Ñ„Ñ–Ñ‡Ð½Ð¸Ð¹ ÑˆÐ»ÑÑ…
        data_directory = 'dataset_256/images'  # Ð¨Ð»ÑÑ… Ð´Ð¾ 256 Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ñƒ
        logging.info(f"ðŸ” Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ”Ñ‚ÑŒÑÑ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ 256 ÐºÐ»Ð°ÑÑ–Ð² Ð· Ð¿Ð°Ð¿ÐºÐ¸: {data_directory}")
    else:
        data_directory = 'dataset/images'  # Original Food101 dataset
        logging.info(f"ðŸ” Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ”Ñ‚ÑŒÑÑ Ð¾Ñ€Ð¸Ð³Ñ–Ð½Ð°Ð»ÑŒÐ½Ð¸Ð¹ Food-101 Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ Ð· Ð¿Ð°Ð¿ÐºÐ¸: {data_directory}")
    
    # Ð¡Ñ‚Ð²Ð¾Ñ€ÐµÐ½Ð½Ñ Ð°Ð±ÑÐ¾Ð»ÑŽÑ‚Ð½Ð¾Ð³Ð¾ ÑˆÐ»ÑÑ…Ñƒ Ð´Ð¾ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ñƒ
    absolute_data_dir = os.path.join(base_path, data_directory)
      # Check for dataset structure (category folders)
    if not os.path.exists(absolute_data_dir):
        # Fallback options for different dataset types
        if dataset_type == "256":
            fallback_paths = [
                'c:\\food_mobilenet\\food_mobilenet\\dataset_256\\images',
                'c:\\food_mobilenet\\food_mobilenet\\dataset_256',
                'c:\\food_mobilenet\\food_mobilenet\\dataset\\images'  # ÐÐ° Ð²Ð¸Ð¿Ð°Ð´Ð¾Ðº, ÑÐºÑ‰Ð¾ 256 Ð² Ð¾ÑÐ½Ð¾Ð²Ð½Ñ–Ð¹ Ð¿Ð°Ð¿Ñ†Ñ–
            ]
        else:
            fallback_paths = [
                'c:\\food_mobilenet\\food_mobilenet\\dataset\\images',
                'c:\\food_mobilenet\\food_mobilenet\\dataset'
            ]
        
        found_dataset = False
        for fallback_path in fallback_paths:
            if os.path.exists(fallback_path):
                absolute_data_dir = fallback_path
                logging.info(f"âœ… Ð—Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ fallback Ð´Ð°Ñ‚Ð°ÑÐµÑ‚: {absolute_data_dir}")
                found_dataset = True
                break
        
        if not found_dataset:
            logging.info(f"âŒ Error: ÐÐµ Ð¼Ð¾Ð¶Ñƒ Ð·Ð½Ð°Ð¹Ñ‚Ð¸ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ Ð´Ð»Ñ Ñ‚Ð¸Ð¿Ñƒ '{dataset_type}'!")
            logging.info(f"Ð¨ÑƒÐºÐ°Ð² Ð·Ð° ÑˆÐ»ÑÑ…Ð°Ð¼Ð¸:")
            logging.info(f"  - {os.path.join(base_path, data_directory)}")
            for path in fallback_paths:
                logging.info(f"  - {path}")
            return
    
    logging.info(f"Using dataset directory: {absolute_data_dir}")
    
    # ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ°, Ñ‡Ð¸ Ñ–ÑÐ½ÑƒÑ” Ñ„Ð°Ð¹Ð» Ð¼Ð¾Ð´ÐµÐ»Ñ–
    if not os.path.isabs(model_path):
        # Ð¯ÐºÑ‰Ð¾ Ð²Ñ–Ð´Ð½Ð¾ÑÐ½Ð¸Ð¹ ÑˆÐ»ÑÑ…, Ñ‚Ð¾ Ñ€Ð¾Ð±Ð¸Ð¼Ð¾ Ð°Ð±ÑÐ¾Ð»ÑŽÑ‚Ð½Ð¸Ð¹
        model_path = os.path.join(base_path, model_path)
    
    if not os.path.exists(model_path):
        logging.info(f"Error: ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð° Ð·Ð° ÑˆÐ»ÑÑ…Ð¾Ð¼: {model_path}")
        logging.info("Ð‘ÑƒÐ´ÑŒ Ð»Ð°ÑÐºÐ°, Ð²ÐºÐ°Ð¶Ñ–Ñ‚ÑŒ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¸Ð¹ ÑˆÐ»ÑÑ… Ð´Ð¾ Ð¼Ð¾Ð´ÐµÐ»Ñ– Ð² ÐºÐ¾Ð´Ñ–.")
        return
    
    model_name = os.path.splitext(os.path.basename(model_path))[0]
    logging.info(f"Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ”Ñ‚ÑŒÑÑ Ð¼Ð¾Ð´ÐµÐ»ÑŒ: {model_path}")
    
    # Data transforms (same as validation in training)
    data_transforms = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])    ])
      # Load entire dataset (all categories and images)
    try:
        logging.info(f"ðŸ“‚ Ð—Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÑƒÑ”Ð¼Ð¾ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ Ð· {dataset_type} ÐºÐ»Ð°ÑÑ–Ð²...")
        logging.info(f"ðŸ“ ÐŸÐ°Ð¿ÐºÐ°: {absolute_data_dir}")
        # Load all images from all categories directly
        full_dataset = datasets.ImageFolder(
            absolute_data_dir,  # Point directly to images folder with categories
            data_transforms
        )
        full_dataloader = DataLoader(
            full_dataset, 
            batch_size=64,  # Smaller batch size for evaluation
            shuffle=False, 
            num_workers=2,
            pin_memory=True        )        
        class_names = full_dataset.classes
        num_classes = len(class_names)
        logging.info(f"Number of classes: {num_classes}")
        logging.info(f"Number of total images: {len(full_dataset)}")
        logging.info(f"Classes found: {class_names[:10]}...")  # Show first 10 classes
        
        # ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´Ð½Ð¾ÑÑ‚Ñ– ÐºÑ–Ð»ÑŒÐºÐ¾ÑÑ‚Ñ– ÐºÐ»Ð°ÑÑ–Ð²
        if num_classes != expected_classes:
            logging.info(f"âš ï¸  Ð£Ð’ÐÐ“Ð: ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð¾Ñ‡Ñ–ÐºÑƒÑ” {expected_classes} ÐºÐ»Ð°ÑÑ–Ð², Ð° Ð² Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ñ– Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ {num_classes}")
            if dataset_type == "256" and num_classes < 256:
                logging.info("ÐœÐ¾Ð¶Ð»Ð¸Ð²Ð¾, Ð¿Ð¾Ñ‚Ñ€Ñ–Ð±ÐµÐ½ Ñ–Ð½ÑˆÐ¸Ð¹ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ Ð°Ð±Ð¾ ÑˆÐ»ÑÑ… Ð´Ð¾ Ð¿Ð°Ð¿ÐºÐ¸ Ð· 256 ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ñ–ÑÐ¼Ð¸")
            elif dataset_type == "101" and num_classes != 101:
                logging.info("ÐœÐ¾Ð¶Ð»Ð¸Ð²Ð¾, Ð¿Ð¾Ñ‚Ñ€Ñ–Ð±ÐµÐ½ Ð¾Ñ€Ð¸Ð³Ñ–Ð½Ð°Ð»ÑŒÐ½Ð¸Ð¹ Food-101 Ð´Ð°Ñ‚Ð°ÑÐµÑ‚")
            logging.info("Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ”Ð¼Ð¾ ÐºÑ–Ð»ÑŒÐºÑ–ÑÑ‚ÑŒ ÐºÐ»Ð°ÑÑ–Ð² Ð· Ð¼Ð¾Ð´ÐµÐ»Ñ– Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð°Ñ€Ñ…Ñ–Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð¸...")
            model_num_classes = expected_classes
        else:
            logging.info(f"âœ… ÐšÑ–Ð»ÑŒÐºÑ–ÑÑ‚ÑŒ ÐºÐ»Ð°ÑÑ–Ð² ÑÐ¿Ñ–Ð²Ð¿Ð°Ð´Ð°Ñ”: {num_classes}")
            model_num_classes = num_classes
        
    except Exception as e:
        logging.info(f"Error loading dataset: {e}")
        return
    
    # Load model
    try:
        model = load_model(model_path, model_num_classes, device)
    except Exception as e:
        logging.info(f"Error loading model: {e}")
        return
    
    # Evaluate model
    try:
        y_pred, y_true, y_probs = evaluate_model(model, full_dataloader, device, class_names)
    except Exception as e:
        logging.info(f"Error during evaluation: {e}")
        return
    
    # Create output directory for results
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    results_dir = os.path.join(base_path, f"evaluation_results_{model_name}_{timestamp}")
    os.makedirs(results_dir, exist_ok=True)
      # Generate plots and save results
    try:
        # Confusion matrix (PNG)
        cm_path = os.path.join(results_dir, "confusion_matrix.png")
        cm = plot_confusion_matrix(y_true, y_pred, class_names, cm_path)
        
        # Confusion matrix (CSV)
        cm_csv_path = os.path.join(results_dir, "confusion_matrix.csv")
        cm_df = save_confusion_matrix_to_csv(y_true, y_pred, class_names, cm_csv_path)
        
        # Top confused classes
        confused_path = os.path.join(results_dir, "top_confused_classes.png")
        plot_top_confused_classes(y_true, y_pred, class_names, save_path=confused_path)
        
        # Per-class accuracy
        accuracy_path = os.path.join(results_dir, "per_class_accuracy.png")
        class_accuracy = plot_class_accuracy(y_true, y_pred, class_names, accuracy_path)
          # Save results to CSV
        csv_path = os.path.join(results_dir, "evaluation_results.csv")
        results_df = save_results_to_csv(y_true, y_pred, class_names, class_accuracy, csv_path)
        
        logging.info(f"\nAll results saved to: {results_dir}")
        logging.info("Generated files:")
        logging.info("  - confusion_matrix.png (Ð²Ñ–Ð·ÑƒÐ°Ð»ÑŒÐ½Ð° Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñ)")
        logging.info("  - confusion_matrix.csv (Ñ‚Ð°Ð±Ð»Ð¸Ñ‡Ð½Ð¸Ð¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚)")
        logging.info("  - evaluation_results.csv (Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ– Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸)")
        logging.info("  - top_confused_classes.png")
        logging.info("  - per_class_accuracy.png")
        logging.info("Evaluation completed successfully!")
        
    except Exception as e:
        logging.info(f"Error generating results: {e}")

if __name__ == "__main__":
    main()
